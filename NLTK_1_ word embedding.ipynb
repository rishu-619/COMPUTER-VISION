{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_3NIusONtqjEQQspz0ZdIcMbLmd6Ex9W",
      "authorship_tag": "ABX9TyOAzy1vxQABGOoOIcQat+ua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishu-619/COMPUTER-VISION/blob/master/NLTK_1_%20word%20embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38G7Q1CB8f44",
        "colab_type": "text"
      },
      "source": [
        "What is NLP?\n",
        "\n",
        "\n",
        "NLP is a part of Artificial Intelligence, developed for the machine to understand human language. The ultimate goal of NLP is to read, understand and make valuable conclusion of human language. It is a very tough job to do as human language has a lot of variation in terms of language, pronunciation etc. Although, in recent times there has been a major breakthrough in the field of NLP.\n",
        "\n",
        "Siri and Alexa are one such example of uses of NLP.\n",
        "\n",
        "We will use NLP for text analytics.\n",
        "\n",
        "There many libraries available for NLP in python. we will focus on NLTK in the later part :\n",
        "\n",
        "Natural Languange Tool Kit (NLTK)\n",
        "Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87WY8UkY8kx6",
        "colab_type": "text"
      },
      "source": [
        "Five main Component of Natural Language processing are:\n",
        "\n",
        "\n",
        "Lexical Analysis − It involves identifying and analyzing the structure of words. Lexicon of a language means the collection of words and phrases in a language. Lexical analysis is dividing the whole chunk of txt into paragraphs, sentences, and words.\n",
        "\n",
        "Syntactic Analysis (Parsing) − It involves analysis of words in the sentence for grammar and arranging words in a manner that shows the relationship among the words. The sentence such as “The movie went to see a family” is rejected by English syntactic analyzer.\n",
        "\n",
        "Semantic Analysis − It draws the exact meaning or the dictionary meaning from the text. The text is checked for meaningfulness. It is done by mapping syntactic structures and objects in the task domain. The semantic analyzer disregards sentence such as “Hot Ice Cream”.\n",
        "\n",
        "Discourse Integration − The meaning of any sentence depends upon the meaning of the sentence just before it. In addition, it also brings about the meaning of immediately succeeding sentence.\n",
        "\n",
        "Pragmatic Analysis − During this, what was said is re-interpreted on what it actually meant. It involves deriving those aspects of language which require real world knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOMSEiiT8tod",
        "colab_type": "text"
      },
      "source": [
        "Advantages of NLP\n",
        "\n",
        "Users can ask questions about any subject and get a direct response within seconds.\n",
        "\n",
        "NLP system provides answers to the questions in natural language\n",
        "NLP system offers exact answers to the questions, no unnecessary or unwanted information\n",
        "\n",
        "The accuracy of the answers increases with the amount of relevant information provided in the question.\n",
        "\n",
        "NLP process helps computers communicate with humans in their language and scales other language-related tasks\n",
        "\n",
        "Allows you to perform more language-based data compares to a human being without fatigue and in an unbiased and consistent way.\n",
        "\n",
        "Structuring a highly unstructured data source\n",
        "\n",
        "Disadvantages of NLP\n",
        "\n",
        "Complex Query Language- the system may not be able to provide the correct answer it the question that is poorly worded or ambiguous.\n",
        "\n",
        "The system is built for a single and specific task only; it is unable to adapt to new domains and problems because of limited functions.\n",
        "\n",
        "NLP system doesn't have a user interface which lacks features that allow users to further interact with the system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgRF_sbRFoIu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "157902a5-5861-476e-fb3a-8846b68a819c"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmtcMZ_KGc24",
        "colab_type": "text"
      },
      "source": [
        "Tokenization\n",
        "\n",
        "Tokenization is a process of breaking down a given paragraph of text into a list of sentence or words. When paragraph is broken down into list of sentences, it is called sentence tokenization. Similarly, if the sentences are further broken down into list of words, it is known as Word tokenization.\n",
        "\n",
        "Let's understand this with an example. Below is a given paragraph, let's see how tokenization works on it:\n",
        "\n",
        "\"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
        "\n",
        "Sentence Tokenize:\n",
        "\n",
        "['India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia.',\n",
        "\n",
        "'It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.',\n",
        "\n",
        "'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.',\n",
        "\n",
        "'In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.']\n",
        "\n",
        "Word tokenize:\n",
        "['India', '(', 'Hindi', ':', 'Bhārat', ')', ',', 'officially', 'the', 'Republic', 'of', 'India', ',', 'is', 'a', 'country', 'in', 'South', 'Asia', '.', 'It', 'is', 'the', 'seventh-largest', 'country', 'by', 'area', ',', 'the', 'second-most', 'populous', 'country', ',', 'and', 'the', 'most', 'populous', 'democracy', 'in', 'the', 'world', '.', 'Bounded', 'by', 'the', 'Indian', 'Ocean', 'on', 'the', 'south', ',', 'the', 'Arabian', 'Sea', 'on', 'the', 'southwest', ',', 'and', 'the', 'Bay', 'of', 'Bengal', 'on', 'the', 'southeast', ',', 'it', 'shares', 'land', 'borders', 'with', 'Pakistan', 'to', 'the', 'west', ';', 'China', ',', 'Nepal', ',', 'and', 'Bhutan', 'to', 'the', 'north', ';', 'and', 'Bangladesh', 'and', 'Myanmar', 'to', 'the', 'east', '.', 'In', 'the', 'Indian', 'Ocean', ',', 'India', 'is', 'in', 'the', 'vicinity', 'of', 'Sri', 'Lanka', 'and', 'the', 'Maldives', ';', 'its', 'Andaman', 'and', 'Nicobar', 'Islands', 'share', 'a', 'maritime', 'border', 'with', 'Thailand', 'and', 'Indonesia', '.']\n",
        "\n",
        "Hope this example clears up the concept of tokenization. We will understand why it is done when we will dive into text analysis.\n",
        "\n",
        "Word Tokenization\n",
        "Example\n",
        "'I am learning Natural Language processing' is being converted into ['I', 'am', 'learning', 'Natural', 'Language', 'processing']\n",
        "Sentence Tokenization\n",
        "Example\n",
        "\"God is Great! I won a lottery.\" is bening converted into [\"God is Great!\", \"I won a lottery\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV6fprDyGoEt",
        "colab_type": "text"
      },
      "source": [
        "##### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7ScBMH7HC7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "632ed0f8-fe0c-4877-bee3-0ab9277a6a23"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_zNQG6VGVqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KcYcTwtGtYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define your text or import from other source\n",
        "text = 'I am learning Natural Language processing'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2MyWAMAG6Of",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ba8a06e-dc4b-4ed7-9b73-e62999cda311"
      },
      "source": [
        "# tokenizing\n",
        "print (word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'learning', 'Natural', 'Language', 'processing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIZ8hqqOHXl2",
        "colab_type": "text"
      },
      "source": [
        "##### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iMNRD16HWJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "#text = \"Good. Morning! How are you?.\"\n",
        "#text = \"Good Morning! How are you\"\n",
        "text = \" Our Company annual growth rate is 25.50%. Good job Mr.Bajaj\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2N_OAUXHcc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f607534c-f660-4513-a7db-cc51c24faa2b"
      },
      "source": [
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' Our Company annual growth rate is 25.50%.', 'Good job Mr.Bajaj']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwBy2khqHhY9",
        "colab_type": "text"
      },
      "source": [
        "# Regular Expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq5r8hTBHh9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRbR5B2nH1zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample text\n",
        "text = \"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have session at 11AM!.We can earn lot of $\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ2PancBH4G-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "056746d1-d45b-4985-bd9d-3b1a9c18bdff"
      },
      "source": [
        "# Print word by word that contains all small case and starts from samll a to z\n",
        "regexp_tokenize(text,\"[a-z]+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['is',\n",
              " 'fun',\n",
              " 'and',\n",
              " 'an',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'texts',\n",
              " 'and',\n",
              " 'sounds',\n",
              " 'but',\n",
              " 'can',\n",
              " 't',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'images',\n",
              " 'e',\n",
              " 'have',\n",
              " 'session',\n",
              " 'at',\n",
              " 'e',\n",
              " 'can',\n",
              " 'earn',\n",
              " 'lot',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loe9WEHCH_va",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "288b0cc9-1ef2-4cd7-8e1d-5400f37ebcfd"
      },
      "source": [
        "# # Print word by word that contains all caps and from caps A to Z\n",
        "regexp_tokenize(text,\"[A-Z]+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'C', 'W', 'AM', 'W']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maBk_qkfIDHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "21b8245f-2dc9-49da-e696-f59a0081e066"
      },
      "source": [
        "# extra quote ' get's you word like can't, don't\n",
        "regexp_tokenize(text,\"[a-z']+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['is',\n",
              " 'fun',\n",
              " 'and',\n",
              " 'an',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'texts',\n",
              " 'and',\n",
              " 'sounds',\n",
              " 'but',\n",
              " \"can't\",\n",
              " 'deal',\n",
              " 'with',\n",
              " 'images',\n",
              " 'e',\n",
              " 'have',\n",
              " 'session',\n",
              " 'at',\n",
              " 'e',\n",
              " 'can',\n",
              " 'earn',\n",
              " 'lot',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjz_HZX5IOcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0f6212bc-89c0-497b-8d15-7c464764a438"
      },
      "source": [
        "# Everything in one line\n",
        "regexp_tokenize(text,\"[\\a-z']+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have session at 11AM!.We can earn lot of $\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7TtOzdoIYUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "40bf3696-9673-4744-e1d0-4a13d313126e"
      },
      "source": [
        "# Anything starts with caret is not equal. \n",
        "regexp_tokenize(text,\"[^a-z']+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' C',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ', ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " '. W',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' 11AM!.W',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' $']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviygxkOIrIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed46b5a5-6a3f-4b0a-cb1d-789fc7c07aa0"
      },
      "source": [
        "# Only numbers\n",
        "regexp_tokenize(text,\"[0-9]+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['11']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0tq6LFoIufP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "aeec6c2a-5286-464f-979d-0ffa767c062f"
      },
      "source": [
        "# Without numbers\n",
        "regexp_tokenize(text,\"[^0-9]+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have session at \",\n",
              " 'AM!.We can earn lot of $']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk7lTXEbIw9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bff242a5-66cf-45be-b2a9-0942b40fc1f5"
      },
      "source": [
        "regexp_tokenize(text,\"[$]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['$']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpEGMe6BJAlg",
        "colab_type": "text"
      },
      "source": [
        "POS (Parts Of Speech)\n",
        "\n",
        "\n",
        "POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition.\n",
        "\n",
        "Example\n",
        "Abbr Meaning\n",
        "CC=> coordinating conjunction\n",
        "CD=> cardinal digit\n",
        "DT=> determiner\n",
        "EX=> existential there\n",
        "FW=> foreign word\n",
        "IN=> preposition/subordinating conjunction\n",
        "JJ=> adjective (large)\n",
        "JJR=> adjective, comparative (larger)\n",
        "JJS=> adjective, superlative (largest)\n",
        "LS=> list market\n",
        "MD=> modal (could, will)\n",
        "NN=> noun, singular (cat, tree)\n",
        "NNS=> noun plural (desks)\n",
        "NNP=> proper noun, singular (sarah)\n",
        "NNPS=> proper noun, plural (indians or americans)\n",
        "PDT=> predeterminer (all, both, half)\n",
        "POS=> possessive ending (parent\\ 's)\n",
        "PRP=> personal pronoun (hers, herself, him,himself)\n",
        "PRP$=> possessive pronoun (her, his, mine, my, our )\n",
        "RB=> adverb (occasionally, swiftly)\n",
        "RBR=> adverb, comparative (greater)\n",
        "RBS=> adverb, superlative (biggest)\n",
        "RP=> particle (about)\n",
        "TO=> infinite marker (to)\n",
        "UH=> interjection (goodbye)\n",
        "VB=> verb (ask)\n",
        "VBG=> verb gerund (judging)\n",
        "VBD=> verb past tense (pleaded)\n",
        "VBN=> verb past participle (reunified)\n",
        "VBP=> verb, present tense not 3rd person singular(wrap)\n",
        "VBZ=> verb, present tense with 3rd person singular (bases)\n",
        "WDT=> wh-determiner (that, what)\n",
        "WP=> wh- pronoun (who)\n",
        "WRB=> wh- adverb (how)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9kVn3n7JC5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "8a879137-1703-486d-dac8-fe1d5d60d744"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "data =' We will see an example of POS tagging.'\n",
        "\n",
        "pos = nltk.pos_tag(nltk.word_tokenize(data))\n",
        "\n",
        "pos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('see', 'VB'),\n",
              " ('an', 'DT'),\n",
              " ('example', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('POS', 'NNP'),\n",
              " ('tagging', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXlyIfoKJZPW",
        "colab_type": "text"
      },
      "source": [
        "Chunking\n",
        "\n",
        "After using parts of speech, Chunking can be used to make data more structured by giving a specific set of rules. Let's understand more about chunking by following example :\n",
        "\n",
        "Use Case of Chunking\n",
        "Chunking is used for entity detection. An entity is that part of the sentence by which machine get the value for any intention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz9maPrMJYOT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "1874d3b7-2018-4ff5-b53d-afdb3cdde6c6"
      },
      "source": [
        "data =' We will see an example of POS tagging.'\n",
        "\n",
        "pos = nltk.pos_tag(nltk.word_tokenize(data))\n",
        "\n",
        "# now once the POS tag has been done. Let's say we want to further structure data such that Nouns are\n",
        "# categorized under one specific node defined by us :\n",
        "\n",
        "my_node = \"MN: {<NNP>*<NN>}\"\n",
        "\n",
        "chunk  =nltk.RegexpParser(my_node)\n",
        "\n",
        "result = chunk.parse(pos)\n",
        "\n",
        "print(result)\n",
        "#result.draw()  # this generates a graphical picture"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  We/PRP\n",
            "  will/MD\n",
            "  see/VB\n",
            "  an/DT\n",
            "  (MN example/NN)\n",
            "  of/IN\n",
            "  (MN POS/NNP tagging/NN)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC5QwgkgK0bD",
        "colab_type": "text"
      },
      "source": [
        "Stop Words\n",
        "\n",
        "\n",
        "Stop words are such words which are very common in occurrence such as ‘a’,’an’,’the’, ‘at’ etc. We ignore such words during the preprocessing part since they do not give any important information and would just take additional space. We can make our custom list of stop words as well if we want. Different libraries have different stop words list. Let’s see the stop words list for NLTK:\n",
        "\n",
        "# import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "​\n",
        "#If you get error download stopwords as below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jybAtZYK2Vi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e5122b81-3a33-435a-eb3b-009acd247755"
      },
      "source": [
        "# import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#If you get error download stopwords as below\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyRhZ6mkN1-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkLAVO6pN6YV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "54eb23cf-b743-48d2-fddf-b8e09bcb1250"
      },
      "source": [
        "print (stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XcIt9IjOBV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1f6534a-f55f-4c1f-c990-a5c749d30610"
      },
      "source": [
        "len(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujch2oY8OJBN",
        "colab_type": "text"
      },
      "source": [
        "#### Similar to the stopwords, we can also ignore punctuations in our sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs00_SOhODjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import string\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5KxtG-ZOMEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c79bc582-f3ee-4e3a-f400-efd6dff85764"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDC7tQbxOOWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove stopwords and punctuations from the above set os texts\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "punct =string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96xnx1jaPKHe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e4b5394-e4ef-4f3d-e789-1f1ab1383771"
      },
      "source": [
        "stop_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu0BXm5GPLrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a6ae6db0-bbc4-4b3b-a785-75649856c56c"
      },
      "source": [
        "# Lets check those punctuations\n",
        "punct"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKOwnGXvPOqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "16965d50-e476-463c-f376-50429ac0410d"
      },
      "source": [
        "#our text\n",
        "text = \"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
        "\n",
        "# Empty list to load clean data\n",
        "cleaned_text = []\n",
        "\n",
        "for word in nltk.word_tokenize(text):\n",
        "    if word not in punct:\n",
        "        if word not in stop_words:\n",
        "            cleaned_text.append(word)\n",
        "    \n",
        "print ('Original Length  == >', len(text))\n",
        "print ('length of cleaned text ==>', len(cleaned_text))\n",
        "print ('\\n',cleaned_text )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Length  == > 614\n",
            "length of cleaned text ==> 57\n",
            "\n",
            " ['India', 'Hindi', 'Bhārat', 'officially', 'Republic', 'India', 'country', 'South', 'Asia', 'It', 'seventh-largest', 'country', 'area', 'second-most', 'populous', 'country', 'populous', 'democracy', 'world', 'Bounded', 'Indian', 'Ocean', 'south', 'Arabian', 'Sea', 'southwest', 'Bay', 'Bengal', 'southeast', 'shares', 'land', 'borders', 'Pakistan', 'west', 'China', 'Nepal', 'Bhutan', 'north', 'Bangladesh', 'Myanmar', 'east', 'In', 'Indian', 'Ocean', 'India', 'vicinity', 'Sri', 'Lanka', 'Maldives', 'Andaman', 'Nicobar', 'Islands', 'share', 'maritime', 'border', 'Thailand', 'Indonesia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbU6zkv7QStN",
        "colab_type": "text"
      },
      "source": [
        "Cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEohsnq_P7mJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7f089223-2727-4b3a-a40f-826b49ede0cc"
      },
      "source": [
        "# Convert into Lower case\n",
        "print (text.lower())\n",
        "\n",
        "# Convert into Upper case\n",
        "print (text.upper())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "india (hindi: bhārat), officially the republic of india, is a country in south asia. it is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. bounded by the indian ocean on the south, the arabian sea on the southwest, and the bay of bengal on the southeast, it shares land borders with pakistan to the west; china, nepal, and bhutan to the north; and bangladesh and myanmar to the east. in the indian ocean, india is in the vicinity of sri lanka and the maldives; its andaman and nicobar islands share a maritime border with thailand and indonesia.\n",
            "INDIA (HINDI: BHĀRAT), OFFICIALLY THE REPUBLIC OF INDIA, IS A COUNTRY IN SOUTH ASIA. IT IS THE SEVENTH-LARGEST COUNTRY BY AREA, THE SECOND-MOST POPULOUS COUNTRY, AND THE MOST POPULOUS DEMOCRACY IN THE WORLD. BOUNDED BY THE INDIAN OCEAN ON THE SOUTH, THE ARABIAN SEA ON THE SOUTHWEST, AND THE BAY OF BENGAL ON THE SOUTHEAST, IT SHARES LAND BORDERS WITH PAKISTAN TO THE WEST; CHINA, NEPAL, AND BHUTAN TO THE NORTH; AND BANGLADESH AND MYANMAR TO THE EAST. IN THE INDIAN OCEAN, INDIA IS IN THE VICINITY OF SRI LANKA AND THE MALDIVES; ITS ANDAMAN AND NICOBAR ISLANDS SHARE A MARITIME BORDER WITH THAILAND AND INDONESIA.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unt8T63nQZcy",
        "colab_type": "text"
      },
      "source": [
        "Stemming\n",
        "\n",
        "Stemming means mapping a group of words to the same stem by removing prefixes or suffixes without giving any value to the “grammatical meaning” of the stem formed after the process.\n",
        "e.g.\n",
        "\n",
        "computation --> comput\n",
        "\n",
        "computer --> comput\n",
        "\n",
        "hobbies --> hobbi\n",
        "\n",
        "We can see that stemming tries to bring the word back to their base word but the base word may or may not have correct grammatical meanings.\n",
        "\n",
        "There are few types of stemmers available in NLTK package. We will talk about popular below two\n",
        "\n",
        "1) Porter Stemmer\n",
        "2) Lancaster Stemmer\n",
        "\n",
        "Let’s see how to use both of them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xFFIhrpQbF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7sIJILVQiHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "880727fd-fbf6-400e-9c9c-5d3e13d7965a"
      },
      "source": [
        "lancaster = LancasterStemmer()\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "Snowball = SnowballStemmer('english')\n",
        "\n",
        "\n",
        "print('Porter stemmer')\n",
        "print(porter.stem(\"hobby\"))\n",
        "print(porter.stem(\"hobbies\"))\n",
        "print(porter.stem(\"computer\"))\n",
        "print(porter.stem(\"computation\"))\n",
        "print(\"**************************\")  \n",
        "\n",
        "print('lancaster stemmer')\n",
        "print(lancaster.stem(\"hobby\"))\n",
        "print(lancaster.stem(\"hobbies\"))\n",
        "print(lancaster.stem(\"computer\"))\n",
        "print(lancaster.stem(\"computation\"))\n",
        "print(\"**************************\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Porter stemmer\n",
            "hobbi\n",
            "hobbi\n",
            "comput\n",
            "comput\n",
            "**************************\n",
            "lancaster stemmer\n",
            "hobby\n",
            "hobby\n",
            "comput\n",
            "comput\n",
            "**************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srhHxZn2Qv3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b77cbf46-15fc-400f-c67c-0833236fc74d"
      },
      "source": [
        "# Lets see with a new sentence\n",
        "\n",
        "sentence = \"I was going to the office on my bike when i saw a car passing by hit the tree.\"\n",
        "\n",
        "token = list(nltk.word_tokenize(sentence))\n",
        "\n",
        "for stemmer in (Snowball, lancaster, porter):\n",
        "    stemm = [stemmer.stem(t) for t in token]\n",
        "    print(\" \".join(stemm))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i was go to the offic on my bike when i saw a car pass by hit the tree .\n",
            "i was going to the off on my bik when i saw a car pass by hit the tre .\n",
            "I wa go to the offic on my bike when i saw a car pass by hit the tree .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF25HXDnRnAk",
        "colab_type": "text"
      },
      "source": [
        "lancaster algorithm is faster than porter but it is more complex.\n",
        "\n",
        "Porter stemmer is the oldest algorithm present and was the most popular to use.\n",
        "\n",
        "Snowball stemmer, also known as  porter2, is the updated version of the Porter stemmer and is currently the most popular stemming algorithm.\n",
        "\n",
        "Snowball stemmer is available for multiple languages as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGBoE29SRo7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5357104e-14b3-4af3-d4ee-115c7bf33a32"
      },
      "source": [
        "# one more simple example of porter\n",
        "print(porter.stem(\"running\"))\n",
        "print(porter.stem(\"runs\"))\n",
        "print(porter.stem(\"ran\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run\n",
            "run\n",
            "ran\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHzTmfaXRutn",
        "colab_type": "text"
      },
      "source": [
        "Lemmatization\n",
        "\n",
        "Lemmatization also does the same thing as stemming and try to bring a word to its base form, but unlike stemming it do keep in account the actual meaning of the base word i.e. the base word belongs to any specific language. The ‘base word’ is known as ‘Lemma’.\n",
        "\n",
        "We use WordNet Lemmatizer for Lemmatization in nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPjmDz6PRskX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kvzOD9iSJc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f9890f20-88ef-4246-b22d-18ff310aab6b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl6ZKa83SEv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7304f37b-7e94-4e45-eed1-15fd4dfb6ab6"
      },
      "source": [
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('running'))\n",
        "print(lemma.lemmatize('runs'))\n",
        "print(lemma.lemmatize('ran'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running\n",
            "run\n",
            "ran\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF4bZRUGSO73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7915bffc-f49d-4acf-bdd6-cd5f526f746c"
      },
      "source": [
        "print(lemma.lemmatize('running',pos='v'))\n",
        "print(lemma.lemmatize('runs',pos='v'))\n",
        "print(lemma.lemmatize('ran',pos='v'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run\n",
            "run\n",
            "run\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMXWM_WaSdaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "9e2b2e8c-aad4-4d73-875f-0122667770b1"
      },
      "source": [
        "# One more example using both stemming and lemma\n",
        "#text = \"studies studying cries cry\"\n",
        "text = \"Bring King Going Anything Sing Ring Nothing Thing\"\n",
        "\n",
        "# Stemming\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer  = PorterStemmer()\n",
        "\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "\n",
        "for w in tokenization:\n",
        "    print (\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w))) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming for Bring is bring\n",
            "Stemming for King is king\n",
            "Stemming for Going is go\n",
            "Stemming for Anything is anyth\n",
            "Stemming for Sing is sing\n",
            "Stemming for Ring is ring\n",
            "Stemming for Nothing is noth\n",
            "Stemming for Thing is thing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAjyhio_Syhl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "7858cf0b-17e7-4eab-e82c-d8315e230406"
      },
      "source": [
        "# Lemma \n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "\n",
        "for w in tokenization:\n",
        "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w))) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemma for Bring is Bring\n",
            "Lemma for King is King\n",
            "Lemma for Going is Going\n",
            "Lemma for Anything is Anything\n",
            "Lemma for Sing is Sing\n",
            "Lemma for Ring is Ring\n",
            "Lemma for Nothing is Nothing\n",
            "Lemma for Thing is Thing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aQSTD50TMIx",
        "colab_type": "text"
      },
      "source": [
        "Wordnet\n",
        "\n",
        "\n",
        "Wordnet is an NLTK corpus reader, a lexical database for English. It can be used to find the meaning of words, synonym or antonym. One can define it as a semantically oriented dictionary of English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D3F1hjoTNOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf7oJ-bbTScl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a8ecbe47-215e-4df8-af8a-404932a79d27"
      },
      "source": [
        "# Synset: It is also called as synonym set or collection of synonym words. \n",
        "# Let us check a example\n",
        "\n",
        "syns = wordnet.synsets(\"cat\")\n",
        "\n",
        "print(syns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), Synset('big_cat.n.01'), Synset('computerized_tomography.n.01'), Synset('cat.v.01'), Synset('vomit.v.01')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HucHiMA-TgrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b7ae4edb-9e8b-4b2f-bc48-ddcaa4dd3a18"
      },
      "source": [
        "# One more example\n",
        "syns = wordnet.synsets(\"cat\")\n",
        "print(syns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), Synset('big_cat.n.01'), Synset('computerized_tomography.n.01'), Synset('cat.v.01'), Synset('vomit.v.01')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfYrU0AlTkkT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "660549a2-d8a4-485e-8fde-c93c7278c918"
      },
      "source": [
        "# Lets find sysnonms and antonyms using python code\n",
        "from nltk.corpus import wordnet\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets(\"active\"):\n",
        "    for l in syn.lemmas():\n",
        "        synonyms.append(l.name())\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print('Synonyms =>',set(synonyms))\n",
        "print('Antonyms =>',set(antonyms))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synonyms => {'active_voice', 'participating', 'dynamic', 'fighting', 'active', 'alive', 'active_agent', 'combat-ready'}\n",
            "Antonyms => {'passive', 'stative', 'passive_voice', 'dormant', 'extinct', 'quiet', 'inactive'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdZarKsOalCN",
        "colab_type": "text"
      },
      "source": [
        "Sentiment-Analysis-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cu8YB1_almX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5481b56e-adc1-4f8f-928d-63539f1d7352"
      },
      "source": [
        "import string\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu0WAXmicx2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text = open('read.txt', 'r',encoding='utf-8')\n",
        "\n",
        "text = open('read.txt', encoding=\"utf-8\").read()\n",
        "\n",
        "lower_case = text.lower()\n",
        "\n",
        "# str.maketrans removes any punctuations \n",
        "\n",
        "cleaned_text = lower_case.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Using word_tokenize to tokenize sentence into words\n",
        "\n",
        "tokenized_words = word_tokenize(cleaned_text, \"english\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PovMjUvWGeb0",
        "colab_type": "text"
      },
      "source": [
        "Sentiment_Analysis 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAtqD1IfGgjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDQ573izG0fD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "1bb4f12f-85bd-4524-fb8e-2ac1850e391c"
      },
      "source": [
        "!pip install GetOldTweets3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GetOldTweets3\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/f4/a00c2a7c90801abc875325bb5416ce9090ac86d06a00cc887131bd73ba45/GetOldTweets3-0.0.11-py3-none-any.whl\n",
            "Collecting pyquery>=1.2.10\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: cssselect, pyquery, GetOldTweets3\n",
            "Successfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo2ybmyxGkaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tweets():\n",
        "    import GetOldTweets3 as got   # library used to scrape data from twitter without any other tools\n",
        "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch('AirIndia') \\\n",
        "        .setSince(\"2020-08-07\") \\\n",
        "        .setUntil(\"2020-08-09\") \\\n",
        "        .setMaxTweets(100)\n",
        "    # Creation of list that contains all tweets\n",
        "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
        "    # Creating list of chosen tweet data\n",
        "    text_tweets = [[tweet.text] for tweet in tweets]\n",
        "    return text_tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-datuHAhGqVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reading text file\n",
        "text = \"\"\n",
        "text_tweets = get_tweets()  # method to get all tweets stored\n",
        "\n",
        "length = len(text_tweets)\n",
        "\n",
        "for i in range(0, length):\n",
        "    text = text_tweets[i][0] + \" \" + text\n",
        "\n",
        "# converting to lowercase\n",
        "lower_case = text.lower()\n",
        "\n",
        "# Removing punctuations\n",
        "cleaned_text = lower_case.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# splitting text into words\n",
        "tokenized_words = cleaned_text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh8RANX2G-4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a6f4516-e79a-48e1-d194-28ef1c127074"
      },
      "source": [
        "# Print texts\n",
        "text_tweets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Dear Mr Harish, our endeavor is to ramp up our operations. Kindly be patient with us. We will be delighted to fly you back. Please follow our twitter handle and Air India website to know more about flight updates.'],\n",
              " ['Air India plane crash: IAF pilot, Sword of Honour recipient Captain DV Sathe among 18 dead in Kerala tragedy - India News '],\n",
              " ['Dear Mr Subhash, we are continuously working on our schedule to match the demands. We will update you soon regarding flights from London to Delhi. Kindly follow our twitter handle and Air India website for the latest flight updates.'],\n",
              " ['@hardeepspuri @airindia please value human life, fear God. May be you dont realise our present/ future has been depending on your decision. But it is understood that you dont bother/ care. Beside tweets, telecasts, interviews, please take steps to resume international flights.'],\n",
              " ['Air India Plane Crash in Kerala Kozhikode..Aviation Minister Hardeep Singh Puri should take responsibility n resign b/c Aviation control should hv advised d pilot to divert and land in Coimbatore instead of #Kozhikode #KozhikodePlaneCrash #HardeepSinghPuri'],\n",
              " ['Un #Boeing 737NG de coste Air India Express, se salió de la pista de aterrizaje del #aeropuerto de #Kozhikode. El vuelo formaba parte del programa de repatriación #VandeBharat. Suben a 18 los muertos en accidente de avión de repatriación en India'],\n",
              " ['Air India Flight Skids and Cracks in Half - Air India Flight Skids and Cracks in Half [ad_1] Flight radar suggested the plane may have circled the airport before trying to land. Survivors told local news media that the aircraft bounced up and... '],\n",
              " ['Air India Flight Skids and Cracks in Half '],\n",
              " ['The reasons that could have led to the Air India Express plane crash in Karipur AAIB, which has started an investigation, will probe various factors, including runway contamination and the role of ATC. Karipur Airport in Kerala '],\n",
              " ['Dubai-Kozhikode Air India flight splits in two on Karipur runway, many i... https://youtu.be/zWxOnIWq2hQ via @YouTube'],\n",
              " [\"It's been more than a week, Airindia website shows tickets are sold out from USA/IAD/ORD to Hyd. When you will add more flights (specifically Delhi to Hyderabad)?\"],\n",
              " ['@HardeepSPuri : Sir, AirIndia has not refunded my cancelled tickets for 6 months. 2day I found out that hundreds of people have not received refunds from Air India in the past year. This seems like a scam. Plz have this reviewed. @airindiain, @MoCA_GoI @PMOIndia @narendramodi'],\n",
              " ['Pilots of Air India\\'s flight from Dubai didn\\'t try to land at any nearby airport as they realized that \"If they fail to land instantly, it may crush at any moment\". I suspect - it was an unfit aircraft &amp; demand enquiry. @DDNewslive @ndtvfeed @PTI_News @AP @ttindia @BBC @TimesNow'],\n",
              " ['What caused the Air India Express crash? Black box retrieved, probe begins '],\n",
              " ['@NZinIndia @MFATNZ @winstonpeters @jacindaardern please approve the private charter @CapaJet from india to nz . We all want to come back and also allow us to book @airindia flights. Not able to book because of agents in btwn. Running out of visa exception as well. Please help us'],\n",
              " [\"HeartBroken to know about the #AirIndia tragedy...imagine the plight of the families who were waiting for their loved ones stranded abroad for months. No point in showing your concern only in words, It's time to come together and do something for them,they need you. Do your bit \"],\n",
              " ['\"Tributes paid to pilots whose actions saved lives in Air India Express crash\" '],\n",
              " ['- What caused the Air India Express crash? Black box retrieved, probe begins Times of India… http://goo.gl/uOzf'],\n",
              " [\"Air India crash: 'I don't want to fly again' \"],\n",
              " ['\"Tributes paid to pilots whose actions saved lives in Air India Express crash.\" #Hero #Thank_A_Hero'],\n",
              " ['Menhub India Minta Semua Pihak Tidak Berspekulasi Atas Jatuhnya Pesawat Air India Express '],\n",
              " ['Shimla’s Table-top Airport ‘Riskiest’, Say Aviation Experts after Air India Plane Crash in Kerala '],\n",
              " ['Air India plane crash: Two Indian expats thank God for missing flight at last moment - Air India plane crash: Two Indian expats thank God for missing flight at last moment -... '],\n",
              " ['UPDATE: At least 16 dead including both pilots, and 123 injured as Air India flight from Dubai with 191 people onboard crash-lands at Calicut Airport '],\n",
              " ['\"Al menos 18 fallecidos por la caída de un avión de Air India Express\" '],\n",
              " ['Please add more vande bharat flights from Delhi to Hyderabad. Or allow commercial flights to Hyderabad. Its been 10 days the flights are sold out on Airindia website.'],\n",
              " ['@airindiain i will take qatar airways from australia to german, then is it ok to catch air india from german to india. i want to travel back to home(india). because i stuck in australia.'],\n",
              " ['International: Al menos 18 personas murieron cuando un avión aterrizó en un aeropuerto en Kerala, India. El avión de Air India Express tenía 190 personas a bordo y se partió en dos después de que se saliera de la… https://www.instagram.com/p/CDoteuJDbJk/?igshid=18bnaitg1psu1'],\n",
              " ['#India: suman 17 muertos y decenas de heridos por accidente aéreo #AirIndia https://www.noticias24.com/suman-17-muertos-y-decenas-de-heridos-en-un-accidente-en-la-india/'],\n",
              " ['एअर इंडिया विमान अपघात: को-पायलट अखिलेश कुमार यांचं पार्थिव दिल्ली विमानतळावर आणलं; उत्तर प्रदेशातल्या मूळगावी मथुरेला नेलं जाणार #AirIndiaCrash #AirIndia #KozikodeAirCrash https://www.lokmat.com/live-news/'],\n",
              " ['Even selling all institutions whether its LIC, RAILWAY, MTNL, AIR INDIA, ETC if it will continue like this. Ye public automatically BJP Ko power se niche gira degi. Or aap bhi dekhna Abhi.'],\n",
              " [\"Investigators recover 'black box' flight recorders from Air India crash https://www.france24.com/en/20200808-investigators-recover-black-box-flight-recorders-from-air-india-crash #SmartNews\"],\n",
              " ['Mortal remains of co-pilot Akhilesh Kumar who lost his life in the #KozhikodeAirCrash arrives at Delhi Airport. Air India Express employees pay tributes to him. His mortal remains are being taken to his native place in Mathura, U.P. @mathurapolice @airindiain'],\n",
              " [\"'Missed Air India flight because of 5-minute delay': How Kerala expat was saved from boarding plane. | #CalicutCrashTwist\"],\n",
              " ['Air India plane crashes in Kerala after skidding off the runway '],\n",
              " ['Air India Express plane crash: Indian consulate in Dubai to remain open to share information, updates '],\n",
              " ['US Academic Urges World Community To Fulfill Its Pledges To Kashmiris; Exposes Indias Designs In IIOJK #LetKashmirSpeak #IOK #Kashmir @AIIndia @UNHumanRights @HRW @JosepBorrellF @mbachelet @antonioguterres @BorisJohnson @amnesty @BBCNews #US #India'],\n",
              " ['Investigators find black box of crashed Air India Express jet, probe begins https://news.familysouq.net/investigators-find-black-box-of-crashed-air-india-express-jet-probe-begins/'],\n",
              " ['Please change picture its showing crash flight of Air India .'],\n",
              " ['@HardeepSPuri jitna extra paisa le kar air india chala rahe ho wo sara nikal gya na khojikhade airport par aab toh samjo kuch sardar ji aur open kar do international flights duniya pareshaan hai apse'],\n",
              " ['What caused the Air India Express crash? Black box retrieved, probe begins '],\n",
              " ['Kamal Haasan, Allu Arjun, Dulquer Salmaan Twitted loss of life in Air India crash..... Several film celebrities from the South took to Twitter to mourn the loss of lives after the plane crashed in kozhikode. '],\n",
              " ['#08Ago | #Internacionales | Un avión de Air India Express con 191 pasajeros a bordo se partió en dos al salirse de la pista de aterrizaje. #CambioWeb #Covid_19 Vía: @El_Cooperante'],\n",
              " ['@airindiain Kolkata - Delhi 21st August flight is cancelled but we are yet to get the money back from Air India. Here is the PNR - Z2TQE. When shall we expect the refund?'],\n",
              " ['Tailwind indicated before Air India 737’s fatal ‘table-top’ overrun https://www.flightglobal.com/safety/tailwind-indicated-before-air-india-737s-fatal-table-top-overrun/139687.article#.Xy8zekkn0SA.twitter @returnoffrank'],\n",
              " [\"RIP Capt. DV Sathe of Air India Express, was a Wing Commander Air Force and had bagged the coveted ‘Sword of Honour’ award. An accomplished fighter pilot. Sathe had turned off the engine before the crash, thus ensuring that it didn't catch fire, which would've been catastrophic. \"],\n",
              " ['https://www.everydaystory40.com/2020/08/eid.html?m=1 #BAYCHE #AirIndia #LOKdown #COVID19'],\n",
              " ['Menhub India Minta Semua Pihak Tidak Berspekulasi Atas Jatuhnya Pesawat Air India Express '],\n",
              " ['Investigators find black box of crashed Air India Express jet, probe begins '],\n",
              " ['No. No word on the refund as yet. They said to allow at least 2-4 weeks. My mom was able to fly on Air India without any problem!!'],\n",
              " ['LOOK: Ambulances arrive at #Kozhikode International Airport, the scene where an Air India Express flight carrying 189 passengers and 6 crew members skidded off a runway during heavy rainfall. '],\n",
              " [\"2020 is full of disaster... 1) COVID-19 2) Beirut Blast 3) Fire in UAE 4) Flood in Mumbai due to heavy rain. 5) Air India Plane crash I don't know how many more to come.... \"],\n",
              " ['The black boxes of an Air India Express jet that crashed on Friday, killing at least 18 people, have been recovered by authorities #Kozhikode'],\n",
              " ['CUATRO NIÑOS ENTRE LAS VICTIMAS DEL ACCIDENTE AÉREO DE AVIÓN DE AIR INDIA @GraficoDelSur'],\n",
              " ['Air India plane crashes in Kerala after skidding off the runway '],\n",
              " ['Air India Flying Returns freezes status expiry and points expiry till December 2020 : https://livefromalounge.com/air-india-flying-returns-status-points-expiry-december-2020/ #AirIndia #CoVid19 #FlyingReturns by Ajay'],\n",
              " ['#IAF #DeepakVasantSathe #AirIndia #AirIndiaplanecrash #AirIndiaCrash'],\n",
              " ['BBC News - Kerala plane crash: 18 dead after Air India plane breaks in two at Calicut https://www.bbc.com/news/world-asia-india-53699857 朝全然ニュース見てなかったからだけどこれは…。雨で無理そうならダイバートするなり戻るなりするんじゃないの？'],\n",
              " ['Delhi: Mortal remains of co-pilot Akhilesh Kumar who lost his life in the flight crash landing incident at #Kozhikode arrives at Delhi Airport. Air India Express employees pay tributes to him. His mortal remains are being taken to his native place in Mathura, Uttar Pradesh. '],\n",
              " ['#malapuram #kozhikodeaircrash #kerala #airindia #planecrash #covid19 #flood #landslide #news #marketingdigital #marketing #marketingstrategy #market #marketingmoves #advertising #advertisement #chaibuoy #digitalmarketingstrategy #digitalmarketingagency #digitaldesign #be_minimal'],\n",
              " ['#NewsTvApp What caused the Air India Express crash? Black box retrieved, probe begins… '],\n",
              " ['So sad to hear this, it must be very hard situation for your parents in a different country with no passports I Hope the Indian Embassy helps to process their passports soon and help you guys! I’m not surprised by the service of Air India’s cabin crew in Vandhe Bharat Flights '],\n",
              " ['\"Al menos 18 fallecidos por la caída de un avión de Air India Express\" '],\n",
              " ['Pics/Videos: Air India flight splits in two in crash landing; deaths reported https://americanmilitarynews.com/2020/08/pics-videos-air-india-flight-splits-in-two-in-crash-landing-deaths-reported/ via @amermilnews'],\n",
              " ['The black box and cockpit voice recorder have been recovered from the site of an#Air #India Express passenger aircraft which crashed near the southern Indian city of Kozhikode state, according to a Civil Aviation official. Find out more here:'],\n",
              " [\"Kerala plane crash: 'Black boxes' from Air India jet found \"],\n",
              " ['Air India plane crashes in Kerala after skidding off the runway '],\n",
              " [\"Investigators recover 'black box' flight recorders from Air India crash http://f24.my/6kxU.T via @FRANCE24\"],\n",
              " ['the brave Mahatar Saheb'],\n",
              " ['72 18 dead, over 100 injured as Air India plane crash-lands '],\n",
              " [\"BBC News - #KeralaIndiaplanecrash: #Blackboxes' from #AirIndia jet found https://www.bbc.com/news/world-asia-india-53706976\"],\n",
              " ['18 dead after Air India plane breaks in two at Calicut en route to Dubai,'],\n",
              " ['#LOÚLTIMO #ÚLTIMAHORA Sobreviviente de avión Air India cuenta su historia '],\n",
              " ['Mueren 16 personas al estrellarse un vuelo de Air India Express con 191 pasajeros a bordo https://www.eldiario.es/1_5de50d a través de @eldiarioes'],\n",
              " ['Autoridades abrirán una investigación sobre el accidente del #Boeing737 de Air India Express que dejó un saldo de 18 muertos y 120 heridos, 22 de gravedad. '],\n",
              " ['Tributes paid to pilots whose actions saved lives in Air India Express crash '],\n",
              " ['\"Tributes paid to pilots whose actions saved lives in Air India Express crash\" '],\n",
              " ['Prayers for the families of those lost in the Air India flight. #AirIndiaCrash'],\n",
              " ['Air India Express announces Rs 10 lakh compensation to family of deceased - Times of India http://dlvr.it/RdGjjh'],\n",
              " ['WTTC among those commenting on Air India Express crash http://dlvr.it/RdGjjn'],\n",
              " ['Air India Express Boeing 737 Crash: Here’s What is Known so Far https://simpleflying.com/air-india-express-crash-update/ .'],\n",
              " [\"Xiyyaarrii Daandii Xiyyaaraa 'Air India Express' jedhamuu namoota 190 fe'ee ture Kibba biyyattii kutaa biyyaa Keraalaatti erga kufe booda yoo xiqqaate namootni 18 lubbuun darbu, anga’oonni himaniiru. Asii dubbisaa: \"],\n",
              " ['\"Tributes paid to pilots whose actions saved lives in Air India Express crash\" '],\n",
              " ['@HardeepSPuri Hello Sir, I have booked Air India Flight for my dependents(Spouse and Daugher) from BOM to EWR and scheduled departure is 15-Aug-2020. Since I am in USA and holding employment till Dec 2021 but my dependent stamping is valid till 20 OCT 2020.'],\n",
              " ['Sobre o acontecimento com o 737 da Air India Express https://youtu.be/MkCvO-BHhGA a través de @YouTube'],\n",
              " ['The AirIndia crash has caused heartbreak and gloom for all of India, but could an issue with the runway have prompted the crash?.'],\n",
              " ['Man who survived Air India plane crash describes terrifying moment of impact which has left at least 18 people dead '],\n",
              " ['Tributes paid to pilots whose actions saved lives in Air India Express crash '],\n",
              " ['- Investigators find black box of crashed Air India Express jet, probe begins Hindustan Times… http://goo.gl/uOzf'],\n",
              " ['This is Indian governments Vande Bharat mission, they should immediately sell Air India, its a disgrace. '],\n",
              " ['- I am sorry for the losses. The aviation industry is primitive. Air India Express is operated by a subsidiary of Air India, owned by the government. #FreeAviationMarket #FreeIndia'],\n",
              " ['@CMOMaharashtra Just think once what u are doing ? One captain( DV Sathe) (Air India) who sacrificed his life to save 174 lives. N you are killing trust of millions of people for saving people who did heinous crime. What will you give answer to bappa? #DishaAndSSRCaseLinked'],\n",
              " ['[OC] Casualties involving Airlines in India by year, following the recent Air India Express Flight 1344 crash on 7 August 2020 https://ift.tt/2F5tE5g August 08, 2020 at 02:31PMhttps://https://ift.tt/33FYwDB'],\n",
              " ['Investigators find black box of crashed Air India Express jet, probe begins '],\n",
              " [\"Kerala plane crash: 'Black boxes' from Air India jet found \"],\n",
              " ['\"Al menos 18 fallecidos por la caída de un avión de Air India Express\" '],\n",
              " [\"Its hard to believe that Dipak Sathe, my friend more than my cousin, is no more. He was pilot of Air India Express carrying passengers from Dubai in 'Vande Bharat Mission', which skidded off the runway at Kozhikode International Airport yesterday night Wha https://www.facebook.com/jeffcely.fernandez/posts/2014643552003437\"],\n",
              " ['Air India Express crash: Investigators find black box data '],\n",
              " [\"Kerala plane crash: 'Black boxes' from Air India jet found \"],\n",
              " ['Air India plane crash live updates: Safety issues were addressed, must refrain from speculative observations, ']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt5WIkj2HBAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
        "              \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n",
        "              \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n",
        "              \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
        "              \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
        "              \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
        "              \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
        "              \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
        "              \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "\n",
        "# Removing stop words from the tokenized words list\n",
        "final_words = [word for word in tokenized_words if word not in stop_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZF9PIinHRKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6281e829-bd5d-4985-a1ec-d06eda138521"
      },
      "source": [
        "# Get emotions text\n",
        "emotion_list = []\n",
        "with open('/content/drive/My Drive/Colab Notebooks/assignment_5/emotions.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        clear_line = line.replace('\\n', '').replace(',', '').replace(\"'\", '').strip()\n",
        "        word, emotion = clear_line.split(':')\n",
        "        if word in final_words:\n",
        "            emotion_list.append(emotion)\n",
        "\n",
        "w = Counter(emotion_list)\n",
        "print(w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({' sad': 3, ' attached': 2, ' fearless': 1, ' lost': 1, ' surprise': 1, ' ecstatic': 1, ' loved': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PC15QhrHje-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "19934580-bfa7-43a5-cc3f-46d57bde5d16"
      },
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "ax1.bar(w.keys(), w.values())\n",
        "fig.autofmt_xdate()\n",
        "plt.savefig('graph.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQer2hOKMwtT",
        "colab_type": "text"
      },
      "source": [
        "Word Embedding\n",
        "\n",
        "Word vectorization is the process of mapping words to a set of real numbers or vectors. This is done to process the given words using machine learning techniques and extract relevant information from them such that it can be used in further predicting words. Vectorization is done by comparing a given word to the corpus(collection) of the available words.\n",
        "\n",
        "It is language modeling and feature learning technique. Word embedding is a way to perform mapping using a neural network.\n",
        "\n",
        "There are various word embedding models available such as word2vec (Google), Glove (Stanford) and fastest (Facebook).\n",
        "\n",
        "We are going to discuss about word2vec in this tutorial\n",
        "\n",
        "Where it is being used\n",
        "\n",
        "Compute similar words:Word embedding is used to suggest similar words to the word being subjected to the prediction model. Along with that it also suggests dissimilar words, as well as most common words.\n",
        "\n",
        "Create a group of related words: It is used for semantic grouping which will group things of similar characteristic together and dissimilar far away.\n",
        "\n",
        "Feature for text classification:Text is mapped into arrays of vectors which is fed to the model for training as well as prediction. Text-based classifier models cannot be trained on the string, so this will convert the text into machine trainable form. Further its features of building semantic help in text-based classification.\n",
        "\n",
        "Document clustering is another application where word embedding is widely used\n",
        "\n",
        "Natural language processing: There are many applications where word embedding is useful and wins over feature extraction phases such as parts of speech tagging, sentimental analysis, and syntactic analysis. Now we have got some knowledge of word embedding. Some light is also thrown on different models to implement word embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikHK8hJ6NcfS",
        "colab_type": "text"
      },
      "source": [
        "Count Vectorizer\n",
        "\n",
        "Count vectorizer uses two of the following models as the base to vectorize the given words on the basis of frequency of words.\n",
        "\n",
        "Bag of Words Model\n",
        "\n",
        "BOW model is used in NLP to represent the given text/sentence/document as a collection (bag) of words without giving any importance to grammar or the occurrence order of the words. It keeps the account of frequency of the words in the text document, which can be used as features in many models.\n",
        "\n",
        "Let’s understand this with an example:\n",
        "\n",
        "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
        "\n",
        "Text2 = “I don’t understand, what is the problem here?”\n",
        "\n",
        "BOW1 = {I :2, went : 1, to : 1,have : 1, a : 1, cup: 1, of :1, coffee : 1, but :1, ended : 1, up :1,having : 1, with :1, her :1}\n",
        "\n",
        "BOW2 = {I : 1, don’t : 1, understand:1, what : 1 , is :1, the : 1, problem : 1, here : 1}\n",
        "\n",
        "BOW is mainly used for feature selection. The above dictionary is converted as a list with only the frequency terms there and on that basis, weights are given to the most occurring terms. But the “stop words” are the most frequent words that appears in raw document. Thus, having a word with high frequency count doesn’t mean that the word is as important. To resolve this problem, “Tf-idf” was introduced. We will discuss about it later.\n",
        "\n",
        "n-gram model\n",
        "\n",
        "As discussed in bag of words model, BOW model doesn’t keep the sequence of words in a given text, only the frequency of words matters. It doesn’t take into account the context of the given sentence, or care for grammatical rules such as verb is following a proper noun in the given text.n-gram model is used in such cases to keep the context of the given text intact. N-gram is the sequence of n words from a given text/document.\n",
        "\n",
        "When, n= 1, we call it a “unigram”.\n",
        "\n",
        "         n=2, it is called a “bigram”. \n",
        "\n",
        "         n=3, it is called a “trigram”.\n",
        "And so on.\n",
        "\n",
        "Let’s understand this with an example:\n",
        "\n",
        "Text1 = “I went to have a cup of coffee but I ended up having lunch with her.”\n",
        "\n",
        "Unigram\n",
        "[I, went, to, have, a, cup, of, coffee, but, I, ended, up, having, lunch, with, her]\n",
        "\n",
        "Bi-gram\n",
        "[I went], [went to],[to have],[have a],[a cup],[cup f],[of coffee],[coffee but],[but I],[I ended],[ended up], [up having],[having lunch],[lunch with],[with her]\n",
        "\n",
        "Tri-gram\n",
        "[I went to], [went to have], [to have a], [have a cup],[ a cup of], [cup of coffee],[ of coffee but],[ coffee but I],[but I ended],[I ended up],[ended up having],[up having lunch],[having lunch with],[lunch with her].\n",
        "\n",
        "Note: We can clearly see that BOW model is nothing but n-gram model when n=1.\n",
        "\n",
        "Skip-grams\n",
        "\n",
        "Skip grams are type of n-grams where the words are not necessarily in the same order as are in the given text i.e. some words can be skipped. Example:\n",
        "\n",
        "Text2 = “I don’t understand, what is the problem here?”\n",
        "\n",
        "1-skip 2-grams (we have to make 2-gram while skipping 1 word)\n",
        "\n",
        "[I understand, don’t what, understand is, what the, is problem, the here].\n",
        "\n",
        "Let's see the implementation of Count vectorizer in python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDXwcgnUM1wR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69a77d6a-f3f6-4e01-ee12-179d86b7670e"
      },
      "source": [
        "# Example of single document\n",
        "# Without stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "# Single document (',' seperates each document)\n",
        "string = [\"This is an example of bag of words!\"]\n",
        "\n",
        "# This step will convert text into tokens \n",
        "vect1 = CountVectorizer()\n",
        "\n",
        "vect1.fit_transform(string)\n",
        "print(\"bag of words :\",vect1.get_feature_names())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bag of words : ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79tEJAGQPG_u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b10762b0-8b99-4e93-a50c-80f2ffb231a2"
      },
      "source": [
        "vect1.vocabulary_"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'an': 0, 'bag': 1, 'example': 2, 'is': 3, 'of': 4, 'this': 5, 'words': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9V3e-69PNC_",
        "colab_type": "text"
      },
      "source": [
        "Fit and transform and predict if the word is present or not\n",
        "This is widely used for document or subject classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T37dulxgPORl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f073666a-b2af-44c6-e8fa-6c7c139ed6f0"
      },
      "source": [
        "c_vect = CountVectorizer()\n",
        "\n",
        "c_vect.fit(string)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdu9USQZP9Se",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8998776e-a3c5-42cf-cb8d-c081b00913bd"
      },
      "source": [
        "string2 = ['Lets understand  bag']\n",
        "\n",
        "c_new_vect = c_vect.transform(string2)\n",
        "\n",
        "print (\"Text Present at \",c_new_vect.toarray())\n",
        "\n",
        "# Compare with the indexes\n",
        "print (\"original indexes\", vect1.get_feature_names() )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text Present at  [[0 1 0 0 0 0 0]]\n",
            "original indexes ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWnnG5MfQg0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "76022123-38f4-484d-ac05-3500257a4301"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gtI_xGVQAZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "a418e0b0-d5ac-4bde-90e8-92e61eca5d60"
      },
      "source": [
        "## Bag Of Words using stopwords (you can avoid writing extra steps to remove stopwords)\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "string = [\"This is an example of bag of words!\"]\n",
        "vect1 = CountVectorizer(stop_words=stop_words)\n",
        "print (vect1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiTfRXo2R2Xg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c4b6047b-bd14-4b3a-e6b7-a25eea1eabf3"
      },
      "source": [
        "vect1.fit_transform(string)\n",
        "print(\"bag of words :\",vect1.get_feature_names())\n",
        "print(\"vocab        :\",vect1.vocabulary_)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bag of words : ['bag', 'example', 'words']\n",
            "vocab        : {'example': 1, 'bag': 0, 'words': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M9hYfHbSNUJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "722ef06a-3017-473a-9bf1-6560913b8cad"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "string = [\"This is an example of n-gram!\"]\n",
        "\n",
        "vect1 = CountVectorizer(ngram_range=(1,1))\n",
        "\n",
        "vect1.fit_transform(string)\n",
        "\n",
        "vect2 = CountVectorizer(ngram_range=(2,2))\n",
        "vect2.fit_transform(string)\n",
        "\n",
        "vect3 = CountVectorizer(ngram_range=(3,3))\n",
        "vect3.fit_transform(string)\n",
        "\n",
        "vect4 = CountVectorizer(ngram_range=(4,4))\n",
        "vect4.fit_transform(string)\n",
        "\n",
        "print(\"1-gram  :\",vect1.get_feature_names())\n",
        "\n",
        "print(\"2-gram  :\",vect2.get_feature_names())\n",
        "print(\"3-gram  :\",vect3.get_feature_names())\n",
        "print(\"4-gram  :\",vect4.get_feature_names())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-gram  : ['an', 'example', 'gram', 'is', 'of', 'this']\n",
            "2-gram  : ['an example', 'example of', 'is an', 'of gram', 'this is']\n",
            "3-gram  : ['an example of', 'example of gram', 'is an example', 'this is an']\n",
            "4-gram  : ['an example of gram', 'is an example of', 'this is an example']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKuRCzFbSYXF",
        "colab_type": "text"
      },
      "source": [
        "Tf-Idf (Term frequency–Inverse document frequency)\n",
        "\n",
        "Wikipedia definition: ” Tf-Idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The Tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf–idf is one of the most popular term-weighting schemes today.”\n",
        "\n",
        "Term Frequency\n",
        "\n",
        "It is simply the frequency in which a word appears in a document in comparison to the total number words in the document. Mathematically given as:\n",
        "\n",
        "Term frequency = (Number of times a word appears in the document) / (Total number of words in the document)\n",
        "\n",
        "Inverse Document Frequency\n",
        "\n",
        "Term frequency has a disadvantage that it tends to give higher weights to words with higher frequency. In such cases words like ‘a’, ‘the’, ‘in’, ’of’ etc. appears more in the documents than other regular words. Thus, more important words are wrongly given lower weights as their frequency is less. To tackle this problem IDF was introduced. IDF decreases the weights of such high frequency terms and increases the weight of terms with rare occurrence. Mathematically it is given as:\n",
        "\n",
        "Inverse Document Frequency = log [(Number of documents)/(Number of documents the word appears in)]\n",
        "\n",
        "note: [log has base 2]\n",
        "\n",
        "Tf-Idf Score = Term frequency * Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWZteppVSc4q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f54920a-ab0c-48a5-f19d-0209d97ad606"
      },
      "source": [
        "import numpy as np \n",
        "np.log2(3)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.584962500721156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El7QtGpJTC4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "8c14d1e1-3f58-4f6b-f69a-6c3f86b88546"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "tfid = TfidfVectorizer(smooth_idf=False)\n",
        "\n",
        "doc= [\"This is an example.\",\"We will see how it works.\",\"IDF can be confusing\"]\n",
        "\n",
        "doc_vector = tfid.fit_transform(doc)\n",
        "#print(tfid.get_feature_names())\n",
        "\n",
        "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
        "df\n",
        "#print(doc_vector)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>an</th>\n",
              "      <th>be</th>\n",
              "      <th>can</th>\n",
              "      <th>confusing</th>\n",
              "      <th>example</th>\n",
              "      <th>how</th>\n",
              "      <th>idf</th>\n",
              "      <th>is</th>\n",
              "      <th>it</th>\n",
              "      <th>see</th>\n",
              "      <th>this</th>\n",
              "      <th>we</th>\n",
              "      <th>will</th>\n",
              "      <th>works</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.408248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    an   be  can  confusing  ...  this        we      will     works\n",
              "0  0.5  0.0  0.0        0.0  ...   0.5  0.000000  0.000000  0.000000\n",
              "1  0.0  0.0  0.0        0.0  ...   0.0  0.408248  0.408248  0.408248\n",
              "2  0.0  0.5  0.5        0.5  ...   0.0  0.000000  0.000000  0.000000\n",
              "\n",
              "[3 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ079EzOUSaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using function\n",
        "def text_matrix(message, countvect):\n",
        "    terms_doc = countvect.fit_transform(message)\n",
        "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names())"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2szAmxDcUVVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "c742cab2-e1d6-416b-f9f6-5ec685e4d8b8"
      },
      "source": [
        "message = ['We are slowly making progress in Natural Language Processing',\n",
        "          \"We will get there\", \"But practice is the only mantra for success\" ]\n",
        "\n",
        "c_vect = CountVectorizer()\n",
        "print (\"Below metrix is the Bag of Words approach\")\n",
        "text_matrix(message, c_vect)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Below metrix is the Bag of Words approach\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>are</th>\n",
              "      <th>but</th>\n",
              "      <th>for</th>\n",
              "      <th>get</th>\n",
              "      <th>in</th>\n",
              "      <th>is</th>\n",
              "      <th>language</th>\n",
              "      <th>making</th>\n",
              "      <th>mantra</th>\n",
              "      <th>natural</th>\n",
              "      <th>only</th>\n",
              "      <th>practice</th>\n",
              "      <th>processing</th>\n",
              "      <th>progress</th>\n",
              "      <th>slowly</th>\n",
              "      <th>success</th>\n",
              "      <th>the</th>\n",
              "      <th>there</th>\n",
              "      <th>we</th>\n",
              "      <th>will</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   are  but  for  get  in  is  ...  slowly  success  the  there  we  will\n",
              "0    1    0    0    0   1   0  ...       1        0    0      0   1     0\n",
              "1    0    0    0    1   0   0  ...       0        0    0      1   1     1\n",
              "2    0    1    1    0   0   1  ...       0        1    1      0   0     0\n",
              "\n",
              "[3 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn7yxUeNUAP8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "2b3a79ed-bb0a-432b-8de3-4637c132f951"
      },
      "source": [
        "# We will call the function created earlier\n",
        "feb_message = [\"What is that covid\",\n",
        "              \"covid is nothing\"]\n",
        "\n",
        "\n",
        "tf = TfidfVectorizer()\n",
        "\n",
        "#Passing same message with TF-IDF\n",
        "\n",
        "text_matrix(feb_message,tf)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>covid</th>\n",
              "      <th>is</th>\n",
              "      <th>nothing</th>\n",
              "      <th>that</th>\n",
              "      <th>what</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.409937</td>\n",
              "      <td>0.409937</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.576152</td>\n",
              "      <td>0.576152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.501549</td>\n",
              "      <td>0.501549</td>\n",
              "      <td>0.704909</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      covid        is   nothing      that      what\n",
              "0  0.409937  0.409937  0.000000  0.576152  0.576152\n",
              "1  0.501549  0.501549  0.704909  0.000000  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaavSCJFUYQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "46f10054-e6e6-4f3f-e99d-56b2fa1e367b"
      },
      "source": [
        "# Importance of Covid increased based on the occurance and total document\n",
        "jul_message = [\"What is that covid covid\",\n",
        "              \"covid is bad\"]\n",
        "\n",
        "text_matrix(jul_message,tf)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bad</th>\n",
              "      <th>covid</th>\n",
              "      <th>is</th>\n",
              "      <th>that</th>\n",
              "      <th>what</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.668501</td>\n",
              "      <td>0.334251</td>\n",
              "      <td>0.469778</td>\n",
              "      <td>0.469778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.704909</td>\n",
              "      <td>0.501549</td>\n",
              "      <td>0.501549</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        bad     covid        is      that      what\n",
              "0  0.000000  0.668501  0.334251  0.469778  0.469778\n",
              "1  0.704909  0.501549  0.501549  0.000000  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sokV7DVUfGx",
        "colab_type": "text"
      },
      "source": [
        "#### Countvectorizer,TF-IDF,n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFL4q87kU39u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "arr = [\"Car was cleaned by Jack\",\n",
        "       \"Jack was cleaned by Car.\"]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5iTORpNUf0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "8a470816-abbb-4aa3-febb-0fc04fb8a15f"
      },
      "source": [
        "# If you want to take into account just term frequencies:\n",
        "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
        "\n",
        "# The ngram range specifies your ngram configuration.\n",
        "\n",
        "X = vectorizer.fit_transform(arr)\n",
        "\n",
        "# Testing the ngram generation:\n",
        "print(\"Feature Names \\n\",vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "print('Array \\n',X.toarray())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature Names \n",
            " ['by car', 'by jack', 'car was', 'cleaned by', 'jack was', 'was cleaned']\n",
            "Array \n",
            " [[0 1 1 1 0 1]\n",
            " [1 0 0 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSfaFxzXVCgv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c5bd8dbf-6337-44c7-a35c-1263df2c0e05"
      },
      "source": [
        "# Testing TFIDF vectorizer without normalization:\n",
        "# You can still specify n-grams here.\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2, 2), norm=None)\n",
        "\n",
        "X = vectorizer.fit_transform(arr)\n",
        "\n",
        "# Testing TFIDF value before normalization:\n",
        "print(X.toarray())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         1.40546511 1.40546511 1.         0.         1.        ]\n",
            " [1.40546511 0.         0.         1.         1.40546511 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}